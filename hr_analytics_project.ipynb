{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "sns.set()\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing data which is in csv format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('HR_Analytics.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start of EDA process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking shape of the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking data types and null count of the columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observation : There is no null or NAN value in the table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lets check how our output variable is distributed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(df['Attrition'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that our output is just Yes or No, so we should do bivariant classification analysis on our dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lets check how all our categorical variable containing variables are distributed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cold=['Attrition','BusinessTravel','Department','EducationField','Gender','JobRole','MaritalStatus','Over18','OverTime']\n",
    "    \n",
    "for i in range(1,9):\n",
    "    print(sns.countplot(df[cold[i]]))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# We can visually see the contents in our categorical columns and how they are distributed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lets visualize all the same using histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.hist(figsize=(15,20),color='g')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that some of the columns are skewed like Yearsatcompany and yearsincurrentrole"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lets visualize our continuous variables with our output variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colc=['Age','DailyRate','DistanceFromHome','Education','EmployeeCount','EmployeeNumber','RelationshipSatisfaction','StandardHours','StockOptionLevel','TotalWorkingYears','TrainingTimesLastYear','WorkLifeBalance','YearsAtCompany','YearsInCurrentRole','YearsSinceLastPromotion','YearsWithCurrManager']\n",
    "     \n",
    "for i in range(0,16):\n",
    "    print(sns.violinplot(x=\"Attrition\", y=df[colc[i]], data=df))\n",
    "    plt.show()\n",
    "    print(sns.stripplot(x=\"Attrition\", y=df[colc[i]], data=df))\n",
    "    plt.show()\n",
    "    print(sns.swarmplot(x=\"Attrition\", y=df[colc[i]], data=df))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Our obsersations based on the above Visualizations\n",
    "We will concentrate only on Yes part of the output variable now\n",
    "\n",
    "1.At age around 30 attrition is high\n",
    "2.People getting daily rate between 250 to 500 have high number of attrition\n",
    "3.People between 0 to 5 in Distancefromhome have maximum numbers of yes for attrition\n",
    "4.Stockoptionlevel with 0 people have maximum number of leaving employees\n",
    "5.Total working years between 8 to 10 also have maximum number of leavers\n",
    "6.Same goes for Training times last year for 2\n",
    "7.Work life balance with number 3 have also left majorly\n",
    "8.People who have worked for 1-2 years has highest number of leavers as per YearsAtCompany column\n",
    "9.Same point as above(8) goes for Years in current role as well\n",
    "10.Years sicne last promotion seems to be high around 1\n",
    "\n",
    "\n",
    "Lets convert all our categorical data to numerical\n",
    "\n",
    "First lets convert our Target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "# Create a label encoder object\n",
    "le = LabelEncoder()\n",
    "le.fit(df['Attrition'])\n",
    "y1=le.transform(df['Attrition'])\n",
    "y1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y=pd.DataFrame(y1,columns =[\"Attrition\"])\n",
    "y.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1=df[['BusinessTravel','Department','EducationField','Gender','JobRole','MaritalStatus','Over18','OverTime']]\n",
    "x1=pd.get_dummies(x1)\n",
    "x1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x2=df[['Age','DailyRate','DistanceFromHome','Education','EmployeeCount','EmployeeNumber','RelationshipSatisfaction','StandardHours','StockOptionLevel','TotalWorkingYears','TrainingTimesLastYear','WorkLifeBalance','YearsAtCompany','YearsInCurrentRole','YearsSinceLastPromotion','YearsWithCurrManager']]\n",
    "x2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final=pd.concat([x2,x1,y], axis=1)\n",
    "df_final.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "corr = df_final.corr()\n",
    "mask = np.zeros_like(corr)\n",
    "mask[np.triu_indices_from(mask)] = True\n",
    "with sns.axes_style(\"white\"):\n",
    "    f, ax = plt.subplots(figsize=(20, 20))\n",
    "    ax = sns.heatmap(corr, mask=mask, vmax=.3, square=True, linewidths=0.5,cmap=\"YlGnBu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1=df_final.corr()\n",
    "#since we only care about the correlation with our output, lets separate it\n",
    "df2=df1.iloc[:,45:46]\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.abs(df2)<0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can drop the columns that are weekly correlated, which are showing as True, i.e, DailyRate, DistanceFromHome,EmployeeNumber,RelationshipSatisfaction,TrainingTimesLastYear,WorkLifeBalance,YearsSinceLastPromotion,BusinessTravel_Non-Travel,BusinessTravel_Travel_Rarely,Department_Human Resources,Department_Research & Development,Department_Sales,EducationField_Human Resources,EducationField_Life Sciences,EducationField_Marketing,EducationField_Medical,EducationField_Other,EducationField_Technical Degree,Gender_Female,Gender_Male,JobRole_Healthcare Representative,JobRole_Human Resources,JobRole_Laboratory Technician,JobRole_Manager,JobRole_Manufacturing Director,JobRole_Research Director,JobRole_Research Scientist,JobRole_Sales Executive,MaritalStatus_Divorced,MaritalStatus_Married"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=df[['Age','Education','EmployeeCount','StandardHours','StockOptionLevel','TotalWorkingYears','YearsAtCompany','YearsInCurrentRole','YearsWithCurrManager','BusinessTravel_Travel_Frequently','JobRole_Sales Representative','MaritalStatus_Single','Over18_Y','OverTime_No','OverTime_Yes']]\n",
    "x.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y=df[['Attrition']]\n",
    "y.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Note: Outliers and skew test was not done as most of the attributes were converted from category\n",
    "\n",
    "We have both of input and output attributes cleaned and in desired format\n",
    "\n",
    "End of EDA Process\n",
    "Lets start Building models to make predictions and find the model that works best on our dataset\n",
    "\n",
    "Start of Machine Learning Process\n",
    "Since out target variable is Bivariant, we are going to do classification analysis\n",
    "\n",
    "Lets import required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import train_test_split,cross_val_score\n",
    "from sklearn.metrics import classification_report,confusion_matrix,accuracy_score,roc_curve,auc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lets split our data randomly and see which model works better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.2,random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lets import all the regression models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "KNN=KNeighborsClassifier(n_neighbors=6)\n",
    "SV=SVC()\n",
    "LR=LogisticRegression()\n",
    "DT=DecisionTreeClassifier(random_state=6)\n",
    "GNB=GaussianNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = []\n",
    "models.append(('KNeighborsClassifier', KNN))\n",
    "models.append(('SVC', SV))\n",
    "models.append(('LogisticRegression', LR))\n",
    "models.append(('DecisionTreeClassifier', DT))\n",
    "models.append(('GaussianNB', GNB))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lets create a loop that will execute all our models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Model = []\n",
    "score = []\n",
    "cvs=[]\n",
    "rocscore=[]\n",
    "for name,model in models:\n",
    "    print('*-----------------------------*',name,'*------------------------------*')\n",
    "    print('\\n')\n",
    "    Model.append(name)\n",
    "    model.fit(x_train,y_train)\n",
    "    print(model)\n",
    "    pre=model.predict(x_test)\n",
    "    print('\\n')\n",
    "    AS=accuracy_score(y_test,pre)\n",
    "    print('Accuracy_score = ',AS)\n",
    "    score.append(AS*100)\n",
    "    print('\\n')\n",
    "    sc = cross_val_score(model, x, y, cv=10, scoring='accuracy').mean()\n",
    "    print('Cross_Val_Score = ',sc)\n",
    "    cvs.append(sc*100)\n",
    "    print('\\n')\n",
    "    false_positive_rate, true_positive_rate, thresholds = roc_curve(y_test,pre)\n",
    "    roc_auc = auc(false_positive_rate, true_positive_rate)\n",
    "    print ('roc_auc_score = ',roc_auc)\n",
    "    rocscore.append(roc_auc*100)\n",
    "    print('\\n')\n",
    "    print('classification_report\\n',classification_report(y_test,pre))\n",
    "    print('\\n')\n",
    "    cm=confusion_matrix(y_test,pre)\n",
    "    print(cm)\n",
    "    print('\\n')\n",
    "    plt.figure(figsize=(10,40))\n",
    "    plt.subplot(911)\n",
    "    plt.title(name)\n",
    "    print(sns.heatmap(cm,annot=True))\n",
    "    plt.subplot(912)\n",
    "    plt.title(name)\n",
    "    plt.plot(false_positive_rate, true_positive_rate, label='AUC = %0.2f'% roc_auc)\n",
    "    plt.plot([0,1],[0,1],'r--')\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.show()\n",
    "    print('\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Final_R = pd.DataFrame({'Classification Model': Model, 'Accuracy Score': score ,'Cross_val_score':cvs,'Roc_auc_curve':rocscore})\n",
    "Final_R"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic Regression seems to be working better with better Cross_val_score and Roc_auc_curve, so lets try to make this better and see if we can increase its accuracy more"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lets first find best random state for Logistic Regression to split our data into train and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_accuracy=0\n",
    "for r_state in range(42,100):\n",
    "    x_train, x_test, y_train, y_test = train_test_split(x,y,random_state = r_state,test_size=0.20)\n",
    "    lr=LogisticRegression()\n",
    "    lr.fit(x_train,y_train)\n",
    "    y_pred = lr.predict(x_test)\n",
    "    accuracy=accuracy_score(y_test,y_pred)\n",
    "    if accuracy>max_accuracy:\n",
    "        max_accuracy=accuracy\n",
    "        final_r_state=r_state\n",
    "print(\"Maximum Accuracy is achived for random state \",final_r_state,\" with a score of\",max_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lets split our dataset based on above random state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.2,random_state=55)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lets find best hyperparameters for Logistic Regression using grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "grid={\"C\":np.logspace(-3,3,7), \"penalty\":[\"l1\",\"l2\"]}\n",
    "lr=LogisticRegression()\n",
    "lr_cv=GridSearchCV(lr,grid,cv=10)\n",
    "lr_cv.fit(x_train,y_train)\n",
    "\n",
    "print(\"tuned hpyerparameters :(best parameters) \",lr_cv.best_params_)\n",
    "print(\"accuracy :\",lr_cv.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Buliding Final Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_f=LogisticRegression(penalty='l2',C=0.1)\n",
    "lr_f.fit(x_train,y_train)\n",
    "pre=lr_f.predict(x_test)\n",
    "accuracy=accuracy_score(y_test,pre)\n",
    "cm=sns.heatmap(confusion_matrix(y_test,pre),annot=True)\n",
    "print(classification_report(y_test, pre))\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve\n",
    "roc_auc = roc_auc_score(y_test, pre)\n",
    "fpr, tpr, thresholds = roc_curve(y_test, lr_f.predict_proba(x_test)[:,1])\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, label='Logistic Regression (area = %0.2f)' % roc_auc)\n",
    "plt.plot([0, 1], [0, 1],'r--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristic')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.savefig('Log_ROC')\n",
    "plt.show()\n",
    "print('Accuracy of the model is',accuracy*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We have achived accuracy of 88.4%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End of Machine learning Process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now lets save our final model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "joblib.dump(lr_f,'HR_Analytics.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
