{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "sns.set()\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('Heart_Disease.csv',header=None)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will give names to the coloumns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns=['age','sex','cp','trestbps','chol','fbs','restecg','thalach','exang','oldpeak','slope','ca','thal','target']\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(df['slope'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(df['ca'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(df['thal'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.cloumns=[['age','sex','cp','trestbps','chol','fbs','restecg','thalach','exang','oldpeak','target']]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lets replace missing value with nan so that it will be easy for us to analyse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.replace({'?': np.nan}, regex=False,inplace=True)\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "first considering the nan values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(subset=[\"trestbps\"], axis=0, inplace=True)\n",
    "df.dropna(subset=[\"oldpeak\"], axis=0, inplace=True)\n",
    "df.dropna(subset=[\"chol\"], axis=0, inplace=True)\n",
    "df.dropna(subset=[\"fbs\"], axis=0, inplace=True)\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conversion of value in target values distiguising the prensence value from the absence value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['target'] = df['target'].replace({2: 1})\n",
    "df['target'] = df['target'].replace({3: 1})\n",
    "df['target'] = df['target'].replace({4: 1})\n",
    "df['target'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, our output will be clear, where 0 will indicate absence and 1 will indicate presence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Converting whole data into a same datatype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df.astype('float')\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reseting the index "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "checking for outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we will check for outliers visually for each attributes, for this box plot is the best option"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col=['age','sex','cp','trestbps','chol','fbs','restecg','thalach','exang','oldpeak','target']\n",
    "for i in range(0,11):\n",
    "    print(sns.boxplot(df[col[i]]))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will romove the outliers as they are huge here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import zscore\n",
    "import numpy as np\n",
    "z = np.abs(zscore(df))\n",
    "print('The shape of the dataset before removing outliers is',df.shape)\n",
    "df_z = df[(z < 3).all(axis=1)]\n",
    "print('The shape of the dataset after removing outliers is',df_z.shape)\n",
    "df=df_z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import zscore\n",
    "import numpy as np\n",
    "z = np.abs(zscore(df))\n",
    "print('The shape of the dataset before removing outliers is',df.shape)\n",
    "df_z = df[(z < 3).all(axis=1)]\n",
    "print('The shape of the dataset after removing outliers is',df_z.shape)\n",
    "df=df_z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculating correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "doing Visualisation with the help of heatmaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = df.corr()\n",
    "mask = np.zeros_like(corr)\n",
    "mask[np.triu_indices_from(mask)] = True\n",
    "with sns.axes_style(\"white\"):\n",
    "    f, ax = plt.subplots(figsize=(12, 12))\n",
    "    ax = sns.heatmap(corr, mask=mask, vmax=.3, square=True,annot=True, linewidths=0.5,cmap=\"YlGnBu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "finding out the coloumns which are not usefull for us as those columns are not corelated with output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1=df.corr()\n",
    "#since we only care about the correlation with our output, lets separate it\n",
    "df2=df1.iloc[:,10:11]\n",
    "df2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the value is 0 there is no correlation, if value is 1 they are highly correlated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets set our threshold value as 0.1 and drop columns with value less than this considering they are not correlated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.abs(df2)<0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above data we can see that chol,restecg,thalach are not correlated with target, hence we can drop them\n",
    "\n",
    "We are using regplot here to visualize the same. There is a line that passes through the points in the plot which will give us an idea if that column is positively correlated or negatively correlated or not correlated at all. If the line comes close to horizontal, we can assume that its not correlated and drop such columns as those columns are not helping us in any way to make our prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col=['age','sex','cp','trestbps','chol','fbs','restecg','thalach','exang','oldpeak','target']\n",
    "\n",
    "for i in range(0,8):\n",
    "    print(sns.regplot(x=col[i],y=\"target\",data=df))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Separating our input and output features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=df[['age','sex','cp','trestbps','fbs','exang','oldpeak']]\n",
    "x.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y=df[['target']]\n",
    "y.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lets check for skewness in our input features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.skew()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lets scale our data using standard scalar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "ss=StandardScaler()\n",
    "ss.fit(x)\n",
    "df_x=ss.transform(x)\n",
    "df_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=pd.DataFrame(df_x,columns=x.columns)\n",
    "x.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have both of input and output attributes cleaned and in desired format\n",
    "\n",
    "End of EDA Process-\n",
    "Lets start Building models to make predictions and find the model that works best on our dataset\n",
    "\n",
    "Start of Machine Learning Process-\n",
    "Since out target variable is Bivariant, we are going to do classification analysis\n",
    "\n",
    "Lets import required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import train_test_split,cross_val_score\n",
    "from sklearn.metrics import classification_report,confusion_matrix,accuracy_score,roc_curve,auc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lets split our data randomly and see which model works better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.2,random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets import all the classification models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "KNN=KNeighborsClassifier(n_neighbors=6)\n",
    "SV=SVC()\n",
    "LR=LogisticRegression()\n",
    "DT=DecisionTreeClassifier(random_state=6)\n",
    "GNB=GaussianNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = []\n",
    "models.append(('KNeighborsClassifier', KNN))\n",
    "models.append(('SVC', SV))\n",
    "models.append(('LogisticRegression', LR))\n",
    "models.append(('DecisionTreeClassifier', DT))\n",
    "models.append(('GaussianNB', GNB))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lets create a loop that will execute all our models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Model = []\n",
    "score = []\n",
    "cvs=[]\n",
    "rocscore=[]\n",
    "for name,model in models:\n",
    "    print('*-----------------------------*',name,'*------------------------------*')\n",
    "    print('\\n')\n",
    "    Model.append(name)\n",
    "    model.fit(x_train,y_train)\n",
    "    print(model)\n",
    "    pre=model.predict(x_test)\n",
    "    print('\\n')\n",
    "    AS=accuracy_score(y_test,pre)\n",
    "    print('Accuracy_score = ',AS)\n",
    "    score.append(AS*100)\n",
    "    print('\\n')\n",
    "    sc = cross_val_score(model, x, y, cv=10, scoring='accuracy').mean()\n",
    "    print('Cross_Val_Score = ',sc)\n",
    "    cvs.append(sc*100)\n",
    "    print('\\n')\n",
    "    false_positive_rate, true_positive_rate, thresholds = roc_curve(y_test,pre)\n",
    "    roc_auc = auc(false_positive_rate, true_positive_rate)\n",
    "    print ('roc_auc_score = ',roc_auc)\n",
    "    rocscore.append(roc_auc*100)\n",
    "    print('\\n')\n",
    "    print('classification_report\\n',classification_report(y_test,pre))\n",
    "    print('\\n')\n",
    "    cm=confusion_matrix(y_test,pre)\n",
    "    print(cm)\n",
    "    print('\\n')\n",
    "    plt.figure(figsize=(10,40))\n",
    "    plt.subplot(911)\n",
    "    plt.title(name)\n",
    "    print(sns.heatmap(cm,annot=True))\n",
    "    plt.subplot(912)\n",
    "    plt.title(name)\n",
    "    plt.plot(false_positive_rate, true_positive_rate, label='AUC = %0.2f'% roc_auc)\n",
    "    plt.plot([0,1],[0,1],'r--')\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.show()\n",
    "    print('\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Final_R = pd.DataFrame({'Classification Model': Model, 'Accuracy Score': score ,'Cross_val_score':cvs,'Roc_auc_curve':rocscore})\n",
    "Final_R"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DecisonTree seems to have higher accuracy, lets try to make this better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf=RandomForestClassifier(n_estimators=100)\n",
    "clf.fit(x_train,y_train)\n",
    "y_pred=clf.predict(x_test)\n",
    "from sklearn import metrics\n",
    "print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred)*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will take DecisionTreeClassifier as final model since accuracy is more"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Now lets save our final model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "joblib.dump(DT,'Heart_Disease.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
